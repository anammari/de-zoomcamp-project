{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 6. Bayes Classification </h2>\n",
    "\n",
    "This notebook has the code for the charts in Chapter 6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT=!gcloud config get-value project\n",
    "PROJECT=PROJECT[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "BUCKET = 'dsongcp_data_lake_de-zoomcamp-prj-375800' # REPLACE WITH YOUR BUCKET NAME if this won't work\n",
    "REGION = 'europe-west6' # REPLACE WITH YOUR BUCKET REGION e.g. us-central1\n",
    "\n",
    "os.environ['BUCKET'] = BUCKET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Exploration using BigQuery </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import google.cloud.bigquery as bigquery\n",
    "\n",
    "bq = bigquery.Client()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "sql = \"\"\"\n",
    "SELECT DISTANCE, DEP_DELAY\n",
    "FROM dsongcp.flights_tzcorr\n",
    "WHERE RAND() < 0.001 AND dep_delay > -20 AND dep_delay < 30 AND distance < 2000\n",
    "\"\"\"\n",
    "df = bq.query(sql).to_dataframe()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "g = sns.jointplot(x=df['DISTANCE'], y=df['DEP_DELAY'], kind=\"hex\", height=10, joint_kws={'gridsize':20})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Set up views in Spark SQL </h3>\n",
    "\n",
    "Start a Spark Session if necessary and get a handle to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Bayes classification using Spark\") \\\n",
    "    .getOrCreate()\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a table definition (this is done lazily; the files won't be read until we issue a query):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "inputs = 'gs://{}/flights/raw/20150*'.format(BUCKET) # SAMPLE\n",
    "#inputs = 'gs://{}/flights/raw/201*'.format(BUCKET)  # FULL\n",
    "flights = spark.read.json(inputs)\n",
    "\n",
    "# this view can now be queried ...\n",
    "flights.createOrReplaceTempView('flights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that distance is STRING\n",
    "print(flights.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example query over the view:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "results = spark.sql('SELECT COUNT(*) FROM flights WHERE dep_delay > -20 AND CAST(distance AS FLOAT) < 2000')\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Restrict to train days </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a CSV file of the training days"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "sql = \"\"\"\n",
    "SELECT *\n",
    "FROM dsongcp.trainday\n",
    "\"\"\"\n",
    "df = bq.query(sql).to_dataframe()\n",
    "df.to_csv('trainday.csv', index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "!head -3 trainday.csv"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "%%bash\n",
    "gsutil cp trainday.csv gs://${BUCKET}/flights/trainday.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataframe of traindays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, BooleanType\n",
    "schema = StructType([\n",
    "    StructField('FL_DATE', StringType(), True),\n",
    "    StructField('is_train_day', BooleanType(), True)\n",
    "])\n",
    "traindays = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv('gs://{}/flights/trainday.csv'.format(BUCKET))\n",
    "\n",
    "traindays.createOrReplaceTempView('traindays')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "results = spark.sql('SELECT * FROM traindays')\n",
    "results.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "statement = \"\"\"\n",
    "SELECT\n",
    "  f.FL_DATE AS date,\n",
    "  CAST(distance AS FLOAT) AS distance,\n",
    "  dep_delay,\n",
    "  IF(arr_delay < 15, 1, 0) AS ontime\n",
    "FROM flights f\n",
    "JOIN traindays t\n",
    "ON f.FL_DATE == t.FL_DATE\n",
    "WHERE\n",
    "  t.is_train_day AND\n",
    "  f.dep_delay IS NOT NULL\n",
    "ORDER BY\n",
    "  f.dep_delay DESC\n",
    "\"\"\"\n",
    "flights = spark.sql(statement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataframe of testdaystemp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField('FL_DATE', StringType(), True),\n",
    "    StructField('is_test_day', BooleanType(), True)\n",
    "])\n",
    "testdaystemp = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv('gs://{}/flights/testdaytemp.csv'.format(BUCKET))\n",
    "\n",
    "testdaystemp.createOrReplaceTempView('testdaystemp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = spark.sql('SELECT * FROM testdaystemp')\n",
    "results.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Hexbin plot </h3>\n",
    "\n",
    "Create a hexbin plot using Spark (repeat of what we did in BigQuery, except that we are now restricting to train days only)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df = flights[(flights['distance'] < 2000) & (flights['dep_delay'] > -20) & (flights['dep_delay'] < 30)]\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample the dataframe so that it fits into memory (not a problem in development, but will be on full dataset); then plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "pdf = df.sample(False, 0.02, 20).toPandas()  # to 100,000 rows approx on complete dataset\n",
    "g = sns.jointplot(x=pdf['distance'], y=pdf['dep_delay'], kind=\"hex\", height=10, joint_kws={'gridsize':20})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Quantization </h3>\n",
    "\n",
    "Now find the quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "distthresh = flights.approxQuantile('distance', list(np.arange(0, 1.0, 0.2)), 0.02)\n",
    "distthresh[-1] = float('inf')\n",
    "distthresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "delaythresh = flights.approxQuantile('dep_delay', list(np.arange(0, 1.0, 0.2)), 0.05)\n",
    "delaythresh[-1] = float('inf')\n",
    "delaythresh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do Bayes in each bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(columns=['dist_thresh', 'delay_thresh', 'frac_ontime'])\n",
    "for m in range(0, 2): # len(distthresh)-1):\n",
    "    for n in range(0, len(delaythresh)-1):\n",
    "        bdf = flights[(flights['distance'] >= distthresh[m])\n",
    "             & (flights['distance'] < distthresh[m+1])\n",
    "             & (flights['dep_delay'] >= delaythresh[n])\n",
    "             & (flights['dep_delay'] < delaythresh[n+1])]\n",
    "        ontime_frac = bdf.agg(F.sum('ontime')).collect()[0][0] / bdf.agg(F.count('ontime')).collect()[0][0]\n",
    "        print (m, n, ontime_frac)\n",
    "        df = df.append({\n",
    "            'dist_thresh': distthresh[m], \n",
    "            'delay_thresh': delaythresh[n],\n",
    "            'frac_ontime': ontime_frac\n",
    "        }, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get close to the 70% threshold only on the last bin.\n",
    "Let's fine tune the delay thresh around the decision boundary.\n",
    "Which we know is on the order of 15 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delaythresh = range(10, 20)\n",
    "\n",
    "df = pd.DataFrame(columns=['dist_thresh', 'delay_thresh', 'frac_ontime'])\n",
    "for m in range(0, len(distthresh)-1):\n",
    "    for n in range(0, len(delaythresh)-1):\n",
    "        bdf = flights[(flights['distance'] >= distthresh[m])\n",
    "             & (flights['distance'] < distthresh[m+1])\n",
    "             & (flights['dep_delay'] >= delaythresh[n])\n",
    "             & (flights['dep_delay'] < delaythresh[n+1])]\n",
    "        ontime_frac = bdf.agg(F.sum('ontime')).collect()[0][0] / bdf.agg(F.count('ontime')).collect()[0][0]\n",
    "        print (m, n, ontime_frac)\n",
    "        df = df.append({\n",
    "            'dist_thresh': distthresh[m], \n",
    "            'delay_thresh': delaythresh[n],\n",
    "            'frac_ontime': ontime_frac\n",
    "        }, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['score'] = abs(df['frac_ontime'] - 0.7)\n",
    "bayes = df.sort_values(['score']).groupby('dist_thresh').head(1).sort_values('dist_thresh')\n",
    "bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes.to_csv('gs://${BUCKET}/flights/bayes.csv'.format(BUCKET), index=False)\n",
    "!gsutil cat gs://{BUCKET}/flights/bayes.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes[ bayes['dist_thresh'] == distthresh[1] ]['delay_thresh'].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model\n",
    "\n",
    "Use the decision threshold to see how well model does on rule-based out-of-sample dataset flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 1\n",
    "statement = \"\"\"\n",
    "SELECT\n",
    "  '{0:.0f}-{1:.0f} miles' AS bin,\n",
    "  ROUND(SUM(IF(dep_delay < {2:f} AND arr_delay < 15, 1, 0))/COUNT(*), 2) AS correct_nocancel,\n",
    "  ROUND(SUM(IF(dep_delay >= {2:f} AND arr_delay < 15, 1, 0))/COUNT(*), 2) AS false_positive,\n",
    "  ROUND(SUM(IF(dep_delay < {2:f} AND arr_delay >= 15, 1, 0))/COUNT(*), 2) AS false_negative,\n",
    "  ROUND(SUM(IF(dep_delay >= {2:f} AND arr_delay >= 15, 1, 0))/COUNT(*), 2) AS correct_cancel,\n",
    "  COUNT(*) AS total_flights\n",
    "FROM flights f\n",
    "JOIN traindays t\n",
    "ON f.FL_DATE == t.FL_DATE\n",
    "WHERE\n",
    "  t.is_train_day == 'False' AND\n",
    "  f.distance >= {0:f} AND f.distance < {1:f}\n",
    "\"\"\".format( distthresh[m], distthresh[m+1], bayes[ bayes['dist_thresh'] == distthresh[m] ]['delay_thresh'].values[0] )\n",
    "print(statement)\n",
    "eval_flights = spark.sql(statement)\n",
    "eval_flights.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distthresh[-1] = 100000 # bug in Spark SQL, which treats 'inf' as a column name\n",
    "for m in range(0, len(distthresh)-1):\n",
    "    statement = \"\"\"\n",
    "SELECT\n",
    "  '{0:.0f}-{1:.0f} miles' AS bin,\n",
    "  ROUND(SUM(IF(dep_delay < {2:f} AND arr_delay < 15, 1, 0))/COUNT(*), 2) AS correct_nocancel,\n",
    "  ROUND(SUM(IF(dep_delay >= {2:f} AND arr_delay < 15, 1, 0))/COUNT(*), 2) AS false_positive,\n",
    "  ROUND(SUM(IF(dep_delay < {2:f} AND arr_delay >= 15, 1, 0))/COUNT(*), 2) AS false_negative,\n",
    "  ROUND(SUM(IF(dep_delay >= {2:f} AND arr_delay >= 15, 1, 0))/COUNT(*), 2) AS correct_cancel,\n",
    "  COUNT(*) AS total_flights\n",
    "FROM flights f\n",
    "JOIN traindays t\n",
    "ON f.FL_DATE == t.FL_DATE\n",
    "WHERE\n",
    "  t.is_train_day == 'False' AND\n",
    "  f.distance >= {0:f} AND f.distance < {1:f}\n",
    "\"\"\".format( distthresh[m], distthresh[m+1], bayes[ bayes['dist_thresh'] == distthresh[m] ]['delay_thresh'].values[0] )\n",
    "eval_flights = spark.sql(statement)\n",
    "eval_flights.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the decision threshold to see how well model does on rule-based temporal test dataset flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 1\n",
    "statement = \"\"\"\n",
    "SELECT\n",
    "  '{0:.0f}-{1:.0f} miles' AS bin,\n",
    "  ROUND(SUM(IF(dep_delay < {2:f} AND arr_delay < 15, 1, 0))/COUNT(*), 2) AS correct_nocancel,\n",
    "  ROUND(SUM(IF(dep_delay >= {2:f} AND arr_delay < 15, 1, 0))/COUNT(*), 2) AS false_positive,\n",
    "  ROUND(SUM(IF(dep_delay < {2:f} AND arr_delay >= 15, 1, 0))/COUNT(*), 2) AS false_negative,\n",
    "  ROUND(SUM(IF(dep_delay >= {2:f} AND arr_delay >= 15, 1, 0))/COUNT(*), 2) AS correct_cancel,\n",
    "  COUNT(*) AS total_flights\n",
    "FROM flights f\n",
    "JOIN testdaystemp t\n",
    "ON f.FL_DATE == t.FL_DATE\n",
    "WHERE\n",
    "  t.is_test_day == 'True' AND\n",
    "  f.distance >= {0:f} AND f.distance < {1:f}\n",
    "\"\"\".format( distthresh[m], distthresh[m+1], bayes[ bayes['dist_thresh'] == distthresh[m] ]['delay_thresh'].values[0] )\n",
    "print(statement)\n",
    "temp_eval_flights = spark.sql(statement)\n",
    "temp_eval_flights.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distthresh[-1] = 100000 # bug in Spark SQL, which treats 'inf' as a column name\n",
    "for m in range(0, len(distthresh)-1):\n",
    "    statement = \"\"\"\n",
    "SELECT\n",
    "  '{0:.0f}-{1:.0f} miles' AS bin,\n",
    "  ROUND(SUM(IF(dep_delay < {2:f} AND arr_delay < 15, 1, 0))/COUNT(*), 2) AS correct_nocancel,\n",
    "  ROUND(SUM(IF(dep_delay >= {2:f} AND arr_delay < 15, 1, 0))/COUNT(*), 2) AS false_positive,\n",
    "  ROUND(SUM(IF(dep_delay < {2:f} AND arr_delay >= 15, 1, 0))/COUNT(*), 2) AS false_negative,\n",
    "  ROUND(SUM(IF(dep_delay >= {2:f} AND arr_delay >= 15, 1, 0))/COUNT(*), 2) AS correct_cancel,\n",
    "  COUNT(*) AS total_flights\n",
    "FROM flights f\n",
    "JOIN testdaystemp t\n",
    "ON f.FL_DATE == t.FL_DATE\n",
    "WHERE\n",
    "  t.is_test_day == 'True' AND\n",
    "  f.distance >= {0:f} AND f.distance < {1:f}\n",
    "\"\"\".format( distthresh[m], distthresh[m+1], bayes[ bayes['dist_thresh'] == distthresh[m] ]['delay_thresh'].values[0] )\n",
    "temp_eval_flights = spark.sql(statement)\n",
    "temp_eval_flights.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Repeat, but on full dataset </h2>\n",
    "\n",
    "You can launch the above processing on the full dataset from within JupyterLab if you want the statistics and graphs updated. I didn't, though, because this is not what I would have really done. Instead, \n",
    "I would have created a standalone Python script and submitted it to the cluster -- there is no need to put JupyterLab in the middle of a production process. We'll submit a standalone Pig program to the cluster in the next section.\n",
    "\n",
    "Steps:\n",
    "<ol>\n",
    "<li> Change the input variable to process all-flights-* </li>\n",
    "<li> Increase cluster size (bash increase_cluster.sh from CloudShell) </li>\n",
    "<li> Clear all Outputs from this notebook </li>\n",
    "<li> Restart Kernel and Run all cells </li>\n",
    "<li> Decrease cluster size (bash decrease_cluster.sh from CloudShell) </li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2019 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
